---
layout: post
title:  "Weeks 9 to 10"
date:   2018-07-23 14:10:51 +0530
categories: GSoC
tags: GSoC
description: Optimistic Concurrency Control, Lock-free parallel BFS with static/dynamic load balancing, Merged Pull Requests, Explored max-flow algorithms.
---
# Tasks completed in weeks 9 to 10

1. Merged Pull Requests.
2. Explored max-flow algorithms.
3. Explored Optimistic Concurrency Control
4. Implemented lock-free parallel Breadth-First Search with static load balancing.
5. WIP: Implementing lock-free parallel Breadth-First Search with dynamic load balancing

# Details

## 1. Merged Pull Requests

As we are nearing the end of GSoC, I have been spending more time getting the code mentioned
in previous posts merged into the main LightGraphs repository. This includes:

1. Parallel Pagerank with degree based partitioning.
2. Parallel Bellman Ford.
3. Improved Dijkstra.
4. Greedy Heuristics.

We decided to break the Greedy Heuristics PR into smaller ones.
Merging 20 different new functions is too troublesome.

## 2. Explored Max-Flow Algorithms

At the time of writing my proposal, I had intended to use atomic operations to produce parallel implementations of Max-Flow finding algorithms such as Edmund-Karp, Dinic and Push-Relabel. 
From experiences explained in previous posts, I know that it would be impossible to obtain 
an improvement in performance by relying on atomic operations.

After discussing this with my mentor, we decided to move on to improving the performance of other core functionalities of LightGraphs.

## 3. Optimistic Concurrency Control

As we have mentioned the previous posts, if multiple processors have read/write or write/write
access to common data, it can lead to unexpected results. As a result, we use locks or atomic
instructions to maintain the correctness of the code. As mentioned in the above section (Explored 
Max-Flow Algorithms), atomic operations and locks degrade the performance of the parallel code and we might end up with code that is slower than the sequential code.

An approach to overcome this obstacle is to **allow data races** to occur and then recover from 
them. i.e., maintain that the **correctness** of the code is **not affected** by the data races
but data races will usually decrease the performance of the program. Hence, when data races are
rare, it can obtain good performance.

### Examples of Race Conditions

1. Interleaving of Machine Level Instruction.
2. Interleaving of Memory Read/Write.
3. Compile Time Code Optimisation.

---
**Interleaving of Machine Level Instruction:** These are race conditions that can occur due to
the fact that a single Julia instruction is broken into multiple machine level instructions 
before execution. For example:

Julia Instruction:

Thread 1: `A += 1`

Thread 2: `A += 1`

Machine Level Instructions:

Thread 1: 
```
Read A into R_1

Increment R_1

Write R_1 into A
```

Thread 2: 
```
Read A into R_2

Increment R_2

Write R_2 into A
```

The programmer may be expecting `A` to be incremented by `2` but find it incremented by `1`.
Since the `A += 1` instruction is broken up into multiple instructions, it is possible 
the first read instruction sets `R_1` and `R_2` to the same value (Initial value of `A`).

---
**Interleaving of Memory Read/Write:** If the data size is greater than the register size then
read/write operations will be borken upto into multiple machine level read/write operations.

For example, If variable `A` is of size `64-bit` and the register size is `32-bit` then a single Julia write on `A` will consist of two machine level writes as a single machine level write can change `32 bits`. 

Consider a machine with 8 bit registers.

Julia Instructions:

Thread 1: ```R_1 = 0xFFFF```

Thread 2: ```R_1 = 0x0000```

Machine Level Instructions:

Thread 1:
```
R_1[1] = 0xFF

R_1[2] = 0xFF
```

Thread 2:
```
R_1[1] = 0x00

R_1[2] = 0x00
```

The programmer may expect `R_1` to have the value `0xFFFF` or `0x0000` but the interleaving
of the instructions could leave it to also have the value `0xFF00` or `0x00FF`.

This should not be an issue because the register size of most computers is 64-bit.
Some computers use 32-bit processors but even then, it is unheard of to use a common computer to process a graph with 2<sup>32</sup>-1 vertices, let alone 2<sup>64</sup>-1 vertices.

Nevertheless, I will be considering this type of data race.

---
**Compile Time Code Optimisation:** During the code optimisation phase of compilation, the compiler
may modify the code to improve performance without considering the effect of other threads
modifying the data. For example,

{% highlight julia %}
A = 1
dists = zeros(Int64, n)
for i in 1:length(dist)
	dists[i] += A
end
{% endhighlight %}

The compiler may modify this during compile time code optimisation as:

{% highlight julia %}
A = 1
dists = zeros(Int64, n)
for i in 1:length(dist)
	dists[i] += 1
end
{% endhighlight %}

Hence, none of the changes made to `A` by other threads will be noted.

Some languages allow data to be marked as `volatile` to inform the compiler that 
the data may be modified externally and the data must be accessed. 
Julia does not have this functionality.

### Assumptions

The **main assumption** I will be making while writing code using optimistic concurrency 
control are:

**I.** If multiple threads write the same value into a variable then at the end of the threaded loop, the variable will have the common value.

For example:

{% highlight julia %}
A = zeros(Int64, 100)
indices = rand(1:100, 1000)
@threads for i in indices
    A[i] = 1
end
{% endhighlight %}
If `i` in `indices` then at the end of the loop `A[i] = 1`.

If `i` in `indices` then at the end of the loop `A[i] = 0`.

This is a reasonable assumption because the programmer always expects the changes made
in the threaded loop to be present after the threaded has been exited.

**II.** If multiple threads have read/write access to a variable and the first read before
preceeds the first write ever then the first read will obtain the correct value.

For example:

{% highlight julia %}
atleast_one_in = false
is_in_if = falses(nthreads())
@threads for thread in 1:nthreads()
    if !atleast_one_in
        atleast_one_in = true
        is_in_if[thread] = true
    end
end
{% endhighlight %}

At the end of the loop, at least one entries in `is_in_if` will be set to true.
This is due to the fact that the first thread to have executed the `if !atleast_one_in`
had performed READ on `atleast_one_in` while no concurrent WRITE was taking place.

This is a reasonable assumption because the programmer always expects a READ operation to 
succeed when a WRITE operation is not occuring concurrently.

## 4. Lock-free parallel BFS with static load balancing

Breadth-First Search is used to find the minimum hop distance (Equivalent to distance with unweighted edges) from some sources to all vertices in a graph.

{% highlight julia %}
function gdistances!(g::AbstractGraph{T}, sources::Vector{<:Integer}) where T<:Integer
   
    n = nv(g)
    vert_level = fill(n, typemax(T)) # Minimum Hop Distances

    visited = falses(n)

    cur_level = Vector{T}()
    next_level = Vector{T}()

    sizehint!(cur_level, n)
    sizehint!(next_level, n)

    @inbounds for s in sources
        push!(cur_level, s)
        vert_level[s] = zero(T)
        visited[s] = true
    end

    n_level = one(T) # The level being explored 
    while !isempty(cur_level)
        @inbounds for v in cur_level
            @inbounds @simd for i in outneighbors(g, v)
                if !visited[i] # vert_level[i] = typemax(T)
                    push!(next_level, i)
                    vert_level[i] = n_level
                    visited[i] = true
                end
            end
        end
        n_level += one(T)
        empty!(cur_level)
        cur_level, next_level = next_level, cur_level
        sort!(cur_level) # For Cache Efficiency 
    end
    return vert_level
end
{% endhighlight %}

As you can see, it explored the graph starting from the `sources` one level at a time.
Vertices of level `L` is the set of vertices (of `g`) that have a minimum hop distance
of `L` from `sources`. 


I thought optimistic concurrency control would be a good way to implement this algorithm because
most graphs are sparse. Hence, the number of data races would be less.

In the parallel implementation, each thread `t` its own `next_level` array (`next_level_t[t]`).
In each iteration of the graph exploration, `cur_level` is partitioned into
`nthreads()` sets. 

Each thread `t` iterates over a vertex `v` in its own partition
and explores `i` in `outneighbors(g, v)`. If `i` has not been explored yet (`!visited[i]`) then the thread pushes `i` into `next_level_t[t]`.

{% highlight julia %}

function unweighted_fast_partition!(num_items::Integer, part::Vector{UnitRange{T}}) where T<:Integer
  
    prefix_sum = 0
    num = length(part)
    for t in 1:num
        left = prefix_sum
        prefix_sum += div(num_items-1+t, num)
        part[t] =  (left+1):prefix_sum
    end

end

function parallel_gdistances_2!(
    g::AbstractGraph{T}, 
    source::Vector{<:Integer}
    ) where T <:Integer
 
    n = nv(g)

    visited = falses(n)
    old_visited = falses(n)
    vert_level = fill(typemax(T), n)

    next_level_t = [Vector{T}(undef, n+1) for _ in 1:nthreads()]
    vert_level = fill(typemax(T), n)
    cur_level = deepcopy(source)
    sizehint!(cur_level, n)

    for t in 1:nthreads()
        next_level_t[t][1] = zero(T)
    end

    for s in source
        visited[s] = old_visited[s] = true
        vert_level[s] = zero(T)
    end

    n_level = one(T)
    partitions = Vector{UnitRange{T}}(undef, nthreads())

    while !isempty(cur_level)
        unweighted_partition!(partitions, length(cur_level))
        
        @threads for ind_set in partitions
            t = threadid()
            @inbounds next_level = next_level_t[t]
            q_rear = zero(T) 
            
            @inbounds for ind in ind_set
                v = cur_level[ind]
                for i in outneighbors(g, v)
                    if !visited[i]
                        visited[i] = true # Race condition
                        next_level[ q_rear+=one(T) ] = i
                    end
                end
            end

            @inbounds next_level[q_rear+one(T)] = zero(T)  
        end

        empty!(cur_level)
        @inbounds for t in 1:nthreads()
            Q = next_level_t[t]

            for v in Q
                (v == zero(T)) && break
                if !old_visited[v] 
                    push!(cur_level, v)
                    old_visited[v] = visited[v] = true
                    vert_level[v] = min(vert_level[v], n_level)
                end
            end
            Q[1] = zero(T)
        end


        n_level += one(T)
       sort!(cur_level, alg = QuickSort)

    end

    return vert_level
end
{% endhighlight %}

#### **Data Race on visited**

Multiple threads have read/write access to `visited::BitArray{1}`.
Hence, it is possible that during a level exploration, multiple threads could have pushed
the same vertex `i` into their queue. That could lead to exploring `i` but it obviously
does not affect the correctness of the code.

{% highlight julia %}
for i in outneighbors(g, v)
    if !visited[i]
        visited[i] = true # Race condition
        next_level[ q_rear+=one(T) ] = i
    end
end
{% endhighlight %}

**It is easy to see that:** The first time in the program the statement `if visited[i]` is being
executed (For any `i` in `vertices(g)`), a write operation on `visited[i]` cannot be occuring
concurrently. This is due to the direct consequence of `assumption II` in section **Optimistic Concurrency Control**.

From the above statement it is easy to prove that `vert_level[i]` will be correct at the end of the program, regardless of any race-conditions occuring on `visited[i]`.

It is possible to avoid data races by providing each thread its own `visited` bit vector.
But that would only reduce performance even more since a vertex that has been explored by one thread 
during a level exploration **will** be considered unexplored by the other threads.

### Benchmarks

---
`nthreads() = 4`

---

`sources = [1,]`

---
---

`g = random_regular_graph(200000, 400)` (\|V\| = 200000, \|E\| = 40000000)

---

`gdistances(g, sources)`:  136.482 ms (9 allocations: 4.60 MiB)

---

`parallel_distances(g, sources)`:  90.917 ms (28 allocations: 10.73 MiB)

---
---
  
--------------------Parallel GDistances------------------------------

`g = loadsnap(:ego_twitter_u)` (\|V\| = 81306, \|E\| = 1342310)

---

`gdistances(g, sources)`:  13.190 ms (8 allocations: 1.87 MiB)

---

`parallel_distances(g, sources)`:  13.645 ms (29 allocations: 4.36 MiB)

---

As you can see, BFS on `g = random_regular_graph(200000, 400)` obtained a good speed up while BFS on
`g = loadsnap(:ego_twitter_u)` showed a negligible slow down. This is likely because the `random_regular_graph` had better partitioning.

## 5. Lock-free parallel BFS with dynamic load balancing


### *Why use Dynamic Loading Balancing?*


If you can recall the `parallel_page_rank` code I obtained a good performance improvement
 using `optimal_weighted_partition` to statically partition `vertices(g)`.

Using `greedy_weighted_partition` or `optimal_weighted_partition` only increased the run-time
of the static load balanced parallel BFS code presented in the previous section.

There are two reasons we cannot use the same strategy.

*Firstly*, BFS is a linear time algorithm and each iteration will need to be partitioned. If partitioning `cur_level` at the beginning of each graph graph exploration phase takes alot of time then we will not obtain any speed up.

*Secondly*, Unlike Page Rank, it is impossible to efficiently estimate the time required to 
iterate over the edges of the vertex. In the page rank algorithm, the time taken to iterate over an edge is more or less constant as the same instructions are executed.

Consider the following block of code:

{% highlight julia %}
@inbounds @simd for i in outneighbors(g, v)
    if !visited[i] # vert_level[i] = typemax(T)
        push!(next_level, i)
        vert_level[i] = n_level
        visited[i] = true
    end
end
{% endhighlight %}

If `visited[i]` is `true` then the time taken to iterate over edge `u -> i` is much lesser
than what it would have been if `visited[i]` is `false`.

### *Broad strategy*

The idea is, each thread `t` will be provided its own `cur_level` array (`cur_level_t[t]`) as well as its own `next_level` array. During each iteration of graph exploration,
each thread `t` will first explore all the vertices in `cur_level_t[t]`.
Once it is finished exploring the vertices in `cur_level_t[t]`, it will iterate over
all the the `cur_level_t` and if it finds an array `cur_level_t[j]` that has unexplored vertices,
it will remove some vertices in `cur_level_t[j]` and then explore them.

I will be refering to the BFS<sub>CL</sub> algorithm presented in the paper title
[Avoiding Locks and Atomic Instructions in Shared-Memory Parallel BFS Using Optimistic Parallelization](https://www.computer.org/csdl/proceedings/ipdpsw/2013/4979/00/4979b628-abs.html).
